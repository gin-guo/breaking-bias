{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":83035,"databundleVersionId":10369658,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":10442279,"sourceType":"datasetVersion","datasetId":6463322},{"sourceId":10982248,"sourceType":"datasetVersion","datasetId":4581967},{"sourceId":118141,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":99348,"modelId":123513},{"sourceId":85979,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":72240,"modelId":76277},{"sourceId":281681,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":241345,"modelId":262985}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"### Import libraries","metadata":{}},{"cell_type":"code","source":"import sys\nimport time\nimport gc\nimport random\nimport numpy as np\nimport pandas as pd\nimport logging\nfrom tqdm import tqdm\nfrom IPython.display import display\n\n# NLP and transformers\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModel\nfrom sentence_transformers import SentenceTransformer, util\n\n# Sklearn\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Set the random seed\nrandom.seed(7)\n\n# Check GPU availability\nif not torch.cuda.is_available():\n    print(\"Sorry - GPU required!\")\n\n# Set logging level for transformers\nlogging.getLogger('transformers').setLevel(logging.ERROR)\n\n# Pandas display options\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T21:37:22.192647Z","iopub.execute_input":"2025-03-10T21:37:22.192928Z","iopub.status.idle":"2025-03-10T21:37:22.199000Z","shell.execute_reply.started":"2025-03-10T21:37:22.192906Z","shell.execute_reply":"2025-03-10T21:37:22.198034Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"### Import input data (topics)","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/llms-you-cant-please-them-all/test.csv\")\nsubmission_df = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv')\n\ntest_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T21:30:54.191972Z","iopub.execute_input":"2025-03-10T21:30:54.192311Z","iopub.status.idle":"2025-03-10T21:30:54.208020Z","shell.execute_reply.started":"2025-03-10T21:30:54.192282Z","shell.execute_reply":"2025-03-10T21:30:54.207343Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"        id  \\\n0  1097671   \n1  1726150   \n2  3211968   \n\n                                                                                           topic  \n0           Compare and contrast the importance of self-reliance and adaptability in healthcare.  \n1  Evaluate the effectiveness of management consulting in addressing conflicts within marketing.  \n2                Discuss the role of self-reliance in achieving success in software engineering.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1097671</td>\n      <td>Compare and contrast the importance of self-reliance and adaptability in healthcare.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1726150</td>\n      <td>Evaluate the effectiveness of management consulting in addressing conflicts within marketing.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3211968</td>\n      <td>Discuss the role of self-reliance in achieving success in software engineering.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"### Load models","metadata":{}},{"cell_type":"code","source":"# Clear GPU memory and delete existing objects if they exist\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\nfor obj in ['model', 'pipe', 'tokenizer']:\n    if obj in globals():\n        del globals()[obj]\n\n# Model configuration\nmodel_name = '/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1'\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n# Load pre-trained word embedding model\nembed_model = SentenceTransformer('/kaggle/input/all-minilm-l6-v2transformers/pytorch/default/1/all-MiniLM-L6-v2', device=\"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T21:32:55.938731Z","iopub.execute_input":"2025-03-10T21:32:55.939055Z","iopub.status.idle":"2025-03-10T21:32:59.464273Z","shell.execute_reply.started":"2025-03-10T21:32:55.939025Z","shell.execute_reply":"2025-03-10T21:32:59.463654Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b2ebe7f7d1f4edbb5cad6787fe9e7c9"}},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"### Configure pipeline","metadata":{}},{"cell_type":"code","source":"# Parameters\nmax_new_tokens = 200  # Maximum length of generated text (can be overridden)\ntemperature = 0.7     # Higher temperature = more random/creative outputs\ntop_p = 0.7           # Nucleus sampling parameter for more diverse outputs (1.0 disables filtering)\n\n# Create pipeline with parameters\npipe = pipeline(\n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer, \n    trust_remote_code=True,\n    max_new_tokens=max_new_tokens,\n    temperature=temperature,\n    top_p=top_p,\n    do_sample=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T21:44:42.984544Z","iopub.execute_input":"2025-03-10T21:44:42.984932Z","iopub.status.idle":"2025-03-10T21:44:42.989840Z","shell.execute_reply.started":"2025-03-10T21:44:42.984902Z","shell.execute_reply":"2025-03-10T21:44:42.988956Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"## Essay generation","metadata":{}},{"cell_type":"markdown","source":"### Define functions","metadata":{}},{"cell_type":"code","source":"def generate_corpus_essay(topic, max_tokens=None):\n    \"\"\"\n    Generates an essay on a given topic using a language model.\n    \n    Args:\n        topic (str): The topic for which the essay is to be generated.\n        max_tokens (int, optional): The maximum number of tokens to generate. \n                                     If None, the model will generate the default number of tokens.\n    \n    Returns:\n        str: The generated essay on the given topic.\n    \"\"\"\n    \n    # Set the prompt for the topic\n    prompt = f\"Write about {topic} in less than 180 words.\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    \n    # Define the generation parameters\n    generation_params = {}\n    if max_tokens:\n        generation_params['max_new_tokens'] = max_tokens\n    \n    # Generate the response\n    output = pipe(messages, **generation_params)[0]\n    essay = output['generated_text'][-1]['content']\n    \n    return essay\n\ndef add_noise_to_text(text, epsilon=0.05):\n    \"\"\"\n    Adds Gaussian noise to the word embeddings of the input text and reconstructs the noisy text.\n    \n    Args:\n        text (str): The input text to which noise will be added.\n        epsilon (float, optional): The standard deviation of the Gaussian noise to apply. Default is 0.05.\n    \n    Returns:\n        str: The noisy version of the input text with added randomness.\n    \"\"\"\n    \n    words = word_tokenize(text)\n    word_embeddings = embed_model.encode(words, convert_to_numpy=True)\n    \n    # Apply Gaussian noise\n    noise = np.random.normal(0, epsilon, word_embeddings.shape)\n    noisy_embeddings = word_embeddings + noise\n    \n    # Compute cosine similarity and find closest words\n    similarities = cosine_similarity(noisy_embeddings, word_embeddings)\n    noisy_words = [words[i] for i in similarities.argmax(axis=1)]\n    \n    # Reconstruct the noisy text\n    noisy_text = \" \".join(noisy_words)\n    return noisy_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T21:47:30.208274Z","iopub.execute_input":"2025-03-10T21:47:30.208632Z","iopub.status.idle":"2025-03-10T21:47:30.214277Z","shell.execute_reply.started":"2025-03-10T21:47:30.208604Z","shell.execute_reply":"2025-03-10T21:47:30.213415Z"}},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":"## Generate essays and output to submission file","metadata":{}},{"cell_type":"code","source":"baseline_essays = []\nnoisy_essays = []\nfor idx, row in test_df.iterrows():\n    essay = generate_corpus_essay(row['topic'], max_new_tokens)\n    baseline_essays.append(essay)\n\n    noisy_essay = add_noise_to_text(essay)\n    noisy_essays.append(noisy_essay)\n    # print(f\"Essay for topic '{row['topic']}':\\n{noisy_essay}\\n{'-'*80}\\n\")\n\n\nsubmission_df = pd.DataFrame({\n    \"id\": test_df[\"id\"],\n    \"essay\": noisy_essays\n})\n\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T21:52:02.618476Z","iopub.execute_input":"2025-03-10T21:52:02.618764Z","iopub.status.idle":"2025-03-10T21:52:47.720361Z","shell.execute_reply.started":"2025-03-10T21:52:02.618741Z","shell.execute_reply":"2025-03-10T21:52:47.719696Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f69bda0a4184238a23b95df8abb1e8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4de409ba99204d2c8a421df033d524a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08fea95219a84900ae2cdb201ec957e4"}},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"print (submission_df['essay'].values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T04:28:01.437668Z","iopub.execute_input":"2025-03-05T04:28:01.437906Z","iopub.status.idle":"2025-03-05T04:28:01.452517Z","shell.execute_reply.started":"2025-03-05T04:28:01.437886Z","shell.execute_reply":"2025-03-05T04:28:01.451858Z"}},"outputs":[],"execution_count":null}]}